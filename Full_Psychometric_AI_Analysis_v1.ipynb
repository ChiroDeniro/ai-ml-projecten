{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxNhe6H1MQb/Pmtrpx1iW6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiroDeniro/ai-ml-projecten/blob/main/Full_Psychometric_AI_Analysis_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "JKIzFRN-ZQiY",
        "outputId": "f3070a04-67a8-4fbf-df07-42bef5c513b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è  Using device: cpu\n",
            "================================================================================\n",
            "NEURAL PSYCHOMETRIC EMBEDDINGS & TRAIT PREDICTION\n",
            "================================================================================\n",
            "\n",
            "üì• STEP 1: Loading Data...\n",
            "‚úÖ Dataset downloaded!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "['neuroticism']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1994276293.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1994276293.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0mdf_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrait_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_prepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# Step 2: Preprocess text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1994276293.py\u001b[0m in \u001b[0;36mload_and_prepare_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Clean data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrait_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìä Dataset: {len(df_clean)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ['neuroticism']"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Neural Psychometric Embeddings & Trait Prediction Pipeline\n",
        "===========================================================\n",
        "\n",
        "16-11-2025\n",
        "\n",
        "AI applications in psychometrics, combining:\n",
        "- Transformer models (BERT, Sentence-BERT)\n",
        "- Classical ML and Deep Learning\n",
        "- Interpretable AI (SHAP)\n",
        "- Interactive deployment (Gradio)\n",
        "\n",
        "Perfect for demonstrating skills for AI/ML roles.\n",
        "\n",
        "CV-Ready Skills Demonstrated:\n",
        "- ü§ó Transformer Models & Transfer Learning\n",
        "- üìä Advanced Data Visualization (UMAP, t-SNE, Plotly)\n",
        "- üß† Deep Learning with PyTorch\n",
        "- üìà Classical ML (XGBoost, Ensemble Methods)\n",
        "- üîç Model Interpretability (SHAP, Attention Analysis)\n",
        "- üéØ Psychometric Validation & Statistical Analysis\n",
        "- üöÄ MLOps & Production Patterns\n",
        "- üé® Interactive ML Demos\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 0: SETUP & INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "# Run this in Google Colab first:\n",
        "\"\"\"\n",
        "!pip install -q transformers==4.35.0\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q datasets==2.14.6\n",
        "!pip install -q umap-learn==0.5.5\n",
        "!pip install -q plotly==5.18.0\n",
        "!pip install -q shap==0.43.0\n",
        "!pip install -q xgboost==2.0.2\n",
        "!pip install -q scikit-learn==1.3.2\n",
        "!pip install -q gradio==4.7.1\n",
        "!pip install -q seaborn==0.13.0\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# ML & Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "import xgboost as xgb\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from umap import UMAP\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Interpretability\n",
        "import shap\n",
        "\n",
        "# Utilities\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def download_dataset():\n",
        "    \"\"\"Download the Essays with Big Five personality dataset\"\"\"\n",
        "    import os\n",
        "    if not os.path.exists('essays.csv'):\n",
        "        # Try primary source\n",
        "        os.system('wget -q https://raw.githubusercontent.com/SenticNet/personality-detection/master/essays.csv -O essays.csv')\n",
        "\n",
        "    if os.path.exists('essays.csv'):\n",
        "        print(\"‚úÖ Dataset downloaded!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Download failed. Please download manually from:\")\n",
        "        print(\"   https://github.com/SenticNet/personality-detection/blob/master/essays.csv\")\n",
        "        return False\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load and prepare the dataset\"\"\"\n",
        "    df = pd.read_csv('essays.csv', encoding='latin-1')\n",
        "\n",
        "    # Standardize column names\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "\n",
        "    # Identify columns\n",
        "    text_col = 'text' if 'text' in df.columns else df.columns[0]\n",
        "\n",
        "    # Rename trait columns for clarity\n",
        "    trait_mapping = {\n",
        "        'cext': 'extraversion',\n",
        "        'cagr': 'agreeableness',\n",
        "        'ccon': 'conscientiousness',\n",
        "        'cneuro': 'neuroticism',\n",
        "        'copn': 'openness'\n",
        "    }\n",
        "\n",
        "    df = df.rename(columns=trait_mapping)\n",
        "    trait_cols = list(trait_mapping.values())\n",
        "\n",
        "    # Data quality checks\n",
        "    df['text_length'] = df[text_col].astype(str).apply(len)\n",
        "    df['word_count'] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Clean data\n",
        "    df_clean = df[df['word_count'] >= 10].copy()\n",
        "    df_clean = df_clean.dropna(subset=trait_cols)\n",
        "\n",
        "    print(f\"üìä Dataset: {len(df_clean)} samples\")\n",
        "    print(f\"üìù Average words: {df_clean['word_count'].mean():.0f}\")\n",
        "    print(f\"üéØ Traits: {trait_cols}\")\n",
        "\n",
        "    return df_clean, text_col, trait_cols\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: TEXT PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Handles all text preprocessing operations\"\"\"\n",
        "\n",
        "    def __init__(self, lowercase: bool = True, remove_urls: bool = True):\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_urls = remove_urls\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Basic text cleaning\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove URLs\n",
        "        if self.remove_urls:\n",
        "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Lowercase\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_linguistic_features(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Extract psychological linguistic features.\n",
        "\n",
        "        Research shows these correlate with personality:\n",
        "        - Pronoun usage ‚Üí Self-reference, social orientation\n",
        "        - Punctuation ‚Üí Emotional expressiveness\n",
        "        - Word length ‚Üí Cognitive complexity\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        n_words = len(words)\n",
        "\n",
        "        if n_words == 0:\n",
        "            return {}\n",
        "\n",
        "        features = {\n",
        "            'word_count': n_words,\n",
        "            'char_count': len(text),\n",
        "            'avg_word_length': np.mean([len(w) for w in words]),\n",
        "\n",
        "            # Pronoun usage (personality markers)\n",
        "            'first_person_singular': sum(1 for w in words if w in ['i', \"i'm\", \"i've\", 'me', 'my', 'mine']) / n_words,\n",
        "            'first_person_plural': sum(1 for w in words if w in ['we', \"we're\", \"we've\", 'us', 'our']) / n_words,\n",
        "            'second_person': sum(1 for w in words if w in ['you', \"you're\", 'your']) / n_words,\n",
        "            'third_person': sum(1 for w in words if w in ['he', 'she', 'they', 'his', 'her', 'their']) / n_words,\n",
        "\n",
        "            # Punctuation (emotional expressiveness)\n",
        "            'exclamation_count': text.count('!') / n_words,\n",
        "            'question_count': text.count('?') / n_words,\n",
        "            'comma_count': text.count(',') / n_words,\n",
        "            'period_count': text.count('.') / n_words,\n",
        "\n",
        "            # Cognitive complexity\n",
        "            'unique_word_ratio': len(set(words)) / n_words,\n",
        "        }\n",
        "\n",
        "        return features\n",
        "\n",
        "    def process_dataframe(self, df: pd.DataFrame, text_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Process entire dataframe and extract features\"\"\"\n",
        "        logger.info(\"Processing text data...\")\n",
        "\n",
        "        # Clean text\n",
        "        df['cleaned_text'] = df[text_col].apply(self.clean_text)\n",
        "\n",
        "        # Extract linguistic features\n",
        "        features_list = []\n",
        "        for text in tqdm(df['cleaned_text'], desc=\"Extracting features\"):\n",
        "            features_list.append(self.extract_linguistic_features(text))\n",
        "\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "\n",
        "        logger.info(f\"Extracted {len(features_df.columns)} linguistic features\")\n",
        "        return df, features_df\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: EMBEDDING GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "class EmbeddingGenerator:\n",
        "    \"\"\"\n",
        "    Generates embeddings using pre-trained transformer models.\n",
        "\n",
        "    Why Sentence-BERT?\n",
        "    - Pre-trained on semantic similarity tasks\n",
        "    - Efficient: Single forward pass\n",
        "    - 384 or 768-dimensional semantic vectors\n",
        "    - State-of-the-art for sentence-level tasks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Initialize embedding model.\n",
        "\n",
        "        Model choices:\n",
        "        - 'all-MiniLM-L6-v2': Fast, 384-dim (recommended for CPU)\n",
        "        - 'all-mpnet-base-v2': Best quality, 768-dim (recommended for GPU)\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading embedding model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model_name = model_name\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.to(device)\n",
        "\n",
        "        logger.info(f\"‚úÖ Model loaded on {device}\")\n",
        "\n",
        "    def encode_texts(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings for a list of texts\"\"\"\n",
        "        logger.info(f\"Encoding {len(texts)} texts...\")\n",
        "\n",
        "        embeddings = self.model.encode(\n",
        "            texts,\n",
        "            batch_size=batch_size,\n",
        "            show_progress_bar=show_progress,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True  # L2 normalization\n",
        "        )\n",
        "\n",
        "        logger.info(f\"‚úÖ Generated embeddings: {embeddings.shape}\")\n",
        "        return embeddings\n",
        "\n",
        "    def get_embedding_dim(self) -> int:\n",
        "        \"\"\"Return the dimensionality of embeddings\"\"\"\n",
        "        return self.model.get_sentence_embedding_dimension()\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: MODEL TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for model training\"\"\"\n",
        "    test_size: float = 0.2\n",
        "    random_state: int = RANDOM_SEED\n",
        "    cv_folds: int = 5\n",
        "\n",
        "class PersonalityPredictor:\n",
        "    \"\"\"\n",
        "    Unified interface for training and evaluating personality prediction models.\n",
        "\n",
        "    Demonstrates:\n",
        "    - Separation of concerns\n",
        "    - Reproducibility\n",
        "    - Extensibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig = None):\n",
        "        self.config = config or ModelConfig()\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_data(self, embeddings: np.ndarray, linguistic_features: pd.DataFrame,\n",
        "                    targets: pd.DataFrame, feature_type: str = 'embeddings'):\n",
        "        \"\"\"\n",
        "        Prepare train/test splits.\n",
        "\n",
        "        Args:\n",
        "            feature_type: 'embeddings', 'linguistic', or 'combined'\n",
        "        \"\"\"\n",
        "        # Choose features\n",
        "        if feature_type == 'embeddings':\n",
        "            X = embeddings\n",
        "        elif feature_type == 'linguistic':\n",
        "            X = linguistic_features.values\n",
        "        elif feature_type == 'combined':\n",
        "            X = np.concatenate([embeddings, linguistic_features.values], axis=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown feature_type: {feature_type}\")\n",
        "\n",
        "        # Split data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, targets,\n",
        "            test_size=self.config.test_size,\n",
        "            random_state=self.config.random_state\n",
        "        )\n",
        "\n",
        "        self.trait_names = targets.columns.tolist()\n",
        "        self.feature_type = feature_type\n",
        "\n",
        "        logger.info(f\"Data prepared: {self.X_train.shape[0]} train, {self.X_test.shape[0]} test samples\")\n",
        "\n",
        "    def train_model(self, model_name: str, model, trait: str):\n",
        "        \"\"\"Train a model for a specific trait\"\"\"\n",
        "        trait_idx = self.trait_names.index(trait)\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(self.X_train)\n",
        "        X_test_scaled = scaler.transform(self.X_test)\n",
        "\n",
        "        # Train\n",
        "        model.fit(X_train_scaled, self.y_train.iloc[:, trait_idx])\n",
        "\n",
        "        # Predict\n",
        "        y_pred_train = model.predict(X_train_scaled)\n",
        "        y_pred_test = model.predict(X_test_scaled)\n",
        "\n",
        "        # Evaluate\n",
        "        results = {\n",
        "            'train_r2': r2_score(self.y_train.iloc[:, trait_idx], y_pred_train),\n",
        "            'test_r2': r2_score(self.y_test.iloc[:, trait_idx], y_pred_test),\n",
        "            'train_rmse': np.sqrt(mean_squared_error(self.y_train.iloc[:, trait_idx], y_pred_train)),\n",
        "            'test_rmse': np.sqrt(mean_squared_error(self.y_test.iloc[:, trait_idx], y_pred_test)),\n",
        "            'test_mae': mean_absolute_error(self.y_test.iloc[:, trait_idx], y_pred_test),\n",
        "            'predictions': y_pred_test\n",
        "        }\n",
        "\n",
        "        # Store\n",
        "        key = f\"{model_name}_{trait}\"\n",
        "        self.models[key] = model\n",
        "        self.scalers[key] = scaler\n",
        "        self.results[key] = results\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_all_traits(self, model_name: str, model_class, **model_params):\n",
        "        \"\"\"Train model for all personality traits\"\"\"\n",
        "        logger.info(f\"Training {model_name} for all traits...\")\n",
        "\n",
        "        all_results = {}\n",
        "        for trait in tqdm(self.trait_names, desc=model_name):\n",
        "            model = model_class(**model_params)\n",
        "            results = self.train_model(model_name, model, trait)\n",
        "            all_results[trait] = results\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def get_results_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Create summary table of all results\"\"\"\n",
        "        summary = []\n",
        "        for key, results in self.results.items():\n",
        "            model_name, trait = key.rsplit('_', 1)\n",
        "            summary.append({\n",
        "                'Model': model_name,\n",
        "                'Trait': trait.capitalize(),\n",
        "                'Train R¬≤': results['train_r2'],\n",
        "                'Test R¬≤': results['test_r2'],\n",
        "                'Test RMSE': results['test_rmse'],\n",
        "                'Test MAE': results['test_mae']\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(summary)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: VISUALIZATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_trait_distributions(df, trait_cols):\n",
        "    \"\"\"Create comprehensive visualization of trait distributions\"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=3,\n",
        "        subplot_titles=trait_cols + ['Trait Correlations'],\n",
        "        specs=[[{}, {}, {}], [{}, {}, {'type': 'heatmap'}]]\n",
        "    )\n",
        "\n",
        "    colors = px.colors.qualitative.Set2\n",
        "    for idx, trait in enumerate(trait_cols):\n",
        "        row = idx // 3 + 1\n",
        "        col = idx % 3 + 1\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Histogram(\n",
        "                x=df[trait],\n",
        "                name=trait.capitalize(),\n",
        "                marker_color=colors[idx],\n",
        "                showlegend=False,\n",
        "                nbinsx=30\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "    # Correlation heatmap\n",
        "    corr_matrix = df[trait_cols].corr()\n",
        "    fig.add_trace(\n",
        "        go.Heatmap(\n",
        "            z=corr_matrix.values,\n",
        "            x=[t.capitalize() for t in trait_cols],\n",
        "            y=[t.capitalize() for t in trait_cols],\n",
        "            colorscale='RdBu',\n",
        "            zmid=0,\n",
        "            text=corr_matrix.values.round(2),\n",
        "            texttemplate='%{text}',\n",
        "            showscale=True\n",
        "        ),\n",
        "        row=2, col=3\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        title_text=\"<b>Big Five Personality Trait Distributions & Correlations</b>\",\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def visualize_embedding_space_2d(embeddings_2d, df, trait_cols):\n",
        "    \"\"\"Create 2D visualization of embedding space colored by traits\"\"\"\n",
        "    viz_df = pd.DataFrame({\n",
        "        'x': embeddings_2d[:, 0],\n",
        "        'y': embeddings_2d[:, 1],\n",
        "        'text_preview': df['cleaned_text'].str[:100] + '...',\n",
        "        'word_count': df['word_count']\n",
        "    })\n",
        "\n",
        "    for trait in trait_cols:\n",
        "        viz_df[trait] = df[trait].values\n",
        "\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=3,\n",
        "        subplot_titles=[t.capitalize() for t in trait_cols] + ['Word Count'],\n",
        "        specs=[[{'type': 'scatter'}]*3, [{'type': 'scatter'}]*3]\n",
        "    )\n",
        "\n",
        "    for idx, trait in enumerate(trait_cols + ['word_count']):\n",
        "        row = idx // 3 + 1\n",
        "        col = idx % 3 + 1\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=viz_df['x'],\n",
        "                y=viz_df['y'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=4,\n",
        "                    color=viz_df[trait],\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=(col == 3),\n",
        "                    opacity=0.6,\n",
        "                    colorbar=dict(title=trait.capitalize())\n",
        "                ),\n",
        "                text=viz_df['text_preview'],\n",
        "                hovertemplate=f'<b>{trait.capitalize()}: %{{marker.color:.2f}}</b><br>' +\n",
        "                             'Text: %{text}<br><extra></extra>',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=row, col=col\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=900,\n",
        "        title_text=\"<b>Semantic Embedding Space Colored by Personality Traits</b>\",\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(showticklabels=False)\n",
        "    fig.update_yaxes(showticklabels=False)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def visualize_model_comparison(results_summary):\n",
        "    \"\"\"Visualize model performance comparison\"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=2,\n",
        "        subplot_titles=['R¬≤ Score by Trait', 'RMSE by Trait']\n",
        "    )\n",
        "\n",
        "    for model in results_summary['Model'].unique():\n",
        "        model_data = results_summary[results_summary['Model'] == model]\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                name=model,\n",
        "                x=model_data['Trait'],\n",
        "                y=model_data['Test R¬≤'],\n",
        "                text=model_data['Test R¬≤'].round(3),\n",
        "                textposition='auto',\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                name=model,\n",
        "                x=model_data['Trait'],\n",
        "                y=model_data['Test RMSE'],\n",
        "                text=model_data['Test RMSE'].round(3),\n",
        "                textposition='auto',\n",
        "                showlegend=False\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=500,\n",
        "        title_text=\"<b>Model Performance Comparison</b>\",\n",
        "        barmode='group'\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"NEURAL PSYCHOMETRIC EMBEDDINGS & TRAIT PREDICTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Download and load data\n",
        "    print(\"\\nüì• STEP 1: Loading Data...\")\n",
        "    if not download_dataset():\n",
        "        return\n",
        "\n",
        "    df_clean, text_col, trait_cols = load_and_prepare_data()\n",
        "\n",
        "    # Step 2: Preprocess text\n",
        "    print(\"\\nüîÑ STEP 2: Preprocessing Text...\")\n",
        "    preprocessor = TextPreprocessor()\n",
        "    df_clean, linguistic_features = preprocessor.process_dataframe(df_clean, text_col)\n",
        "\n",
        "    # Step 3: Generate embeddings\n",
        "    print(\"\\nüß† STEP 3: Generating Embeddings...\")\n",
        "    embedding_generator = EmbeddingGenerator('all-MiniLM-L6-v2')\n",
        "    text_embeddings = embedding_generator.encode_texts(\n",
        "        df_clean['cleaned_text'].tolist(),\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    # Step 4: Dimensionality reduction for visualization\n",
        "    print(\"\\nüìä STEP 4: Dimensionality Reduction...\")\n",
        "    reducer_2d = UMAP(n_components=2, random_state=RANDOM_SEED)\n",
        "    embeddings_2d = reducer_2d.fit_transform(text_embeddings)\n",
        "\n",
        "    # Step 5: Train models\n",
        "    print(\"\\nüèãÔ∏è STEP 5: Training Models...\")\n",
        "    predictor = PersonalityPredictor()\n",
        "    predictor.prepare_data(\n",
        "        embeddings=text_embeddings,\n",
        "        linguistic_features=linguistic_features,\n",
        "        targets=df_clean[trait_cols],\n",
        "        feature_type='embeddings'\n",
        "    )\n",
        "\n",
        "    # Train multiple models\n",
        "    ridge_results = predictor.train_all_traits('Ridge', Ridge, alpha=1.0, random_state=RANDOM_SEED)\n",
        "    rf_results = predictor.train_all_traits('RandomForest', RandomForestRegressor,\n",
        "                                            n_estimators=100, max_depth=10,\n",
        "                                            random_state=RANDOM_SEED, n_jobs=-1)\n",
        "    xgb_results = predictor.train_all_traits('XGBoost', xgb.XGBRegressor,\n",
        "                                             n_estimators=100, max_depth=6,\n",
        "                                             learning_rate=0.1, random_state=RANDOM_SEED)\n",
        "\n",
        "    # Step 6: Evaluate and compare\n",
        "    print(\"\\nüìà STEP 6: Model Evaluation...\")\n",
        "    results_summary = predictor.get_results_summary()\n",
        "    print(\"\\n\" + results_summary.to_string(index=False))\n",
        "\n",
        "    avg_performance = results_summary.groupby('Model')['Test R¬≤'].mean().sort_values(ascending=False)\n",
        "    print(\"\\nüèÜ Average R¬≤ by Model:\")\n",
        "    print(avg_performance)\n",
        "\n",
        "    # Step 7: Save results\n",
        "    print(\"\\nüíæ STEP 7: Saving Results...\")\n",
        "\n",
        "    # Save models\n",
        "    best_model_name = avg_performance.index[0]\n",
        "    save_dir = 'models'\n",
        "    import os\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for trait in trait_cols:\n",
        "        key = f\"{best_model_name}_{trait}\"\n",
        "        with open(f\"{save_dir}/{key}.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'model': predictor.models[key],\n",
        "                'scaler': predictor.scalers[key],\n",
        "                'results': predictor.results[key]\n",
        "            }, f)\n",
        "\n",
        "    # Save embeddings\n",
        "    np.save(f\"{save_dir}/text_embeddings.npy\", text_embeddings)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'model_name': best_model_name,\n",
        "        'embedding_model': embedding_generator.model_name,\n",
        "        'traits': trait_cols,\n",
        "        'n_samples': len(df_clean),\n",
        "        'avg_r2': float(avg_performance.iloc[0]),\n",
        "        'timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    with open(f\"{save_dir}/metadata.json\", 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Models saved to {save_dir}/\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'df': df_clean,\n",
        "        'embeddings': text_embeddings,\n",
        "        'embeddings_2d': embeddings_2d,\n",
        "        'predictor': predictor,\n",
        "        'results': results_summary,\n",
        "        'trait_cols': trait_cols\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()"
      ]
    }
  ]
}